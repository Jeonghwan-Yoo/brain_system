/*
하나에 정통한 엔지니어보다는 여러 분야에 다방면으로 정통한 엔지니어를 요구하고 있다.
그 분야에 대해 초소한의 전문성을 갖춰야 한다.

01 메모리 계층(Memory Hierarchy)
개발자입장에서는 메모리가 가장 중요한 요소이다.
제한된 환경(CPU 속도는 결정된 사항)에서 가급적 높은 성능을 낼 수 있도록 프로그래밍하기 위해서는
메모리의 특성을 잘 파악하고 있어야 한다.

메모리의 범위와 종류
보통 메모리라고 하면 메인 메모리에 해당하는 RAM을 생각한다.
램이 사용되는 메인 메모리 이외에도 메모리라고 불릴 수 있는 요소들이 많다.
컴퓨터를 구성하는 요소 중에서 임시적이든, 영구적이든 저장 기능을 가지고 있으면 무조건 메모리 범위.

[메인 메모리]
가장 먼저 떠올릴 수 있ㄴ은 것은 메인 메모리인 RAM이다. 정확히 말하면 DRAM 계열의 메모리이다. 참고로 메인
메모리가 반드시 램이어야 할 이유는 없다. 그래서 메인 메모리 = 램은 아니지만 일반적이다.
[Register]
CPU 안에 내장되어 있어 연산을 위한 저장소를 제공한다.
[Cache]
중요성을 인식하기 시작한 것이 캐쉬메모리이다. 캐쉬는 DRAM 보다 빠른 SRAM으로 구성하는데, 캐쉬 메모리를 캐쉬라.
캐쉬는 CPU와 램 사이에서 중간 저장소 역할을 하는 메모리이다. 캐쉬 메모리는 원래 CPU의 일부로 존재하는 메모리
개념이 아니다. CPU에 근접해 있는 메모리 개념. CPU의 일부로 존재하는 메모리는 레지스터이다.
캐쉬는 CPU에 가장 근접해 있는 메모리이다. CPU를 디자인하는 과정에서 함께 디자인되어 하나로 제품화 된다.
[하드디스크와 이외의 저장 장치들]
하드디스크는 크고 작은 파일들을 저장하기 위한 용도로도 사용되지만, 프로그램 실행에 있어서도 중요한 의미.
그 밖에도 SD카드, CD-ROM과 같은 I/O 장치들도 메모리에 해당한다.
즉 "프로그래머는 레지스터, 캐쉬, 메인 메모리, 하드디스크뿐 아니라 그 밖의 I/O 장치들과의 입출력 타이밍 및
대기 시간 등을 가장 중요한 요소로 생각하고 항상 고민해야 한다."

메모리 계층 구조
프로그램이 실행되는 동안에 메모리가 하는 역할은 데이터의 입력 및 출력이다.
기본적인 역할은 모든 메모리가 동일하다.
가장 큰 차이점은 CPU를 기준으로 얼마나 멀리 떨어져 있느냐이다.
CPU와 가까이에 있을수록 빠르고, 멀리 있을수록 느리다.
레지스터 < 캐쉬 메모리 < 메인 메모리 < 하드디스크
CPU의 레지스터 접근은 별다른 절차가 필요 없다.
그러나 메인 메모리에 접근하기 위해서는 복잡한 과정을 거쳐야 한다.
대표적인 것이 버스 인터페이스 컨트롤이다.
데이터를 입력 및 출력하기 위해서 메모리 버스를 거쳐야 하기 때문에 그만큼 더 느리다.
하지만 CPU 근처로 대용량의 메모리를 가져갈수록 기술문제와 비용도 훨씬 많이 든다.
레지스터는 차지하고 있는 크기가 가장 작지만 가장 빠르다.
그 다음 L1캐쉬가 CPU에 보다 근접해 있고 L2가 있다.
그 아래에는 메인 메모리가 존재해 크지만 상대적으로 느리다.
가장 하단에 존재하는 것이 하드디스크이다.
하드디스크에 있는 내용은 프로그램의 실행을 위해서 메인 메모리로 이동한다.
메인 메모리에 있는 데이터 일부도 실행을 위해 L2 캐쉬로 이동한다.
L2캐쉬에 있는 데이터 이부는 L1캐쉬로 이동하고, 연산에 필요한 데이터는 레지스터로 이동한다.
즉 모든 메모리의 역할일 피라미드 구조에서 자신보다 아래에 있는 메모리를 캐쉬(자주 사용되는 메모리의 일부를
저장해서 속도를 향상시키는 것을 의미)하기 위해 존재.
데이터가 없다면 마지막에 하드디스크에서 읽어 들이게 된다.
생각보다 데이터의 이동이 심해 접근하는 빈도수가 많아 느려진다고 생각할 수 있다.
그러나 시스템에서는 캐쉬 메모리가 높은 성능 향상을 가져다 준다.
그만큼 연산에 필요한 데이터가 캐쉬메모리에 존재할 확률이 아주 높다는 뜻.
메인 메모리를 제외한 L1캐쉬와 L2캐쉬에 연산에 필요한 데이터가 존재할 확률이 90%이상 된다.
그래서 결론적으로는 캐쉬는 속도를 향상시킨다.

Level 1 캐쉬와 Level 2 캐쉬
역사적으로 보면, L1 캐쉬는 CPU 내부가 아닌 시스템 보드(메인보드)엥 존재하는 메모리였다.
이것이 성능 향상을 위해 CPU 내부로 들어가게 된 것이다. 그리고 CPU 외부에 또 다른 캐쉬가 등장했다.(L2)
시스템의 성능을 좌우하는 클럭속도는 항상 느린쪽에 맞춰지게 되어 있다. CPU는 고속화되었지만, 그 속도만큼
메인 메모리의 처리속도가 따라가지 못하고 있다. 결국 CPU와 메인 메모리 속도 차가 커지게 되었다.
아무리 CPU 속도가 두 배 빨라져도 주변장치의 속도가 그대로라면, 결코 기대하는 만큼의 속도 향상을 경험할 수 없다.
CPU가 연산을 하기 위해서는 메인 메모리에서부터 피연산자에 해당하는 데이터를 가져와야 하고, 또 그 연산결과를
메모리에 저장한 다음에야 비로소 다음 작업을 할 수 있다. 즉 프로그램이 실행되는 과정에서 많은 시간을 데이터 입력
및 출력에 할애.
                         병목현상 발생지역
|ALU<-->레지스터<-->캐쉬|<---------------->|메인메모리|
ALU는 연산을 위해서 레지스터에 저장된 데이터를 가져와야 하는데, 필요한 데이터가 존재하지 않을 경우 메인 메모리에서
데이터를 읽어와야 한다. 따라서 자주 사용되는 주소 번지의 데이터는 캐쉬 메모리에 저장해둬서 메인메모리까지 가는
빈도수를 줄여주고 있다.
만약 CPU가 1Ghz로 동작한다고 하고 데이터가 이미 캐쉬메모리에 올라와 있다면 1Ghz로 처리될 것이다.
그러나 모든 데이터가 캐쉬 메모리에 있을 수 없어 메인 메모리에서 데이터를 가져오면 0.2Ghz로 동작하기 때문에
메인 메모리가 데이터를 전송해 줄 때까지 CPU는 쉬고 있어야 한다. 그러면 병목현상이 발생한다.
병목현상을 최소화하려면 캐쉬 사이즈를 최대한 키우고 레지스터 크기를 최대한 키우는 것인데 불가능하다.
그래서 캐쉬를 하나 더 두는 것이다.
|ALU<-->레지스터<-->L1캐쉬<-->L2캐쉬|<------>|메인메모리|
L2는 과거 시스템 보드에 존재하다가 오늘날에는 CPU 내부로 들어갔다.
그래서 병목 현상의 주체가 L1캐쉬에서 L2캐쉬로 이동했다. L2 캐쉬는 CPU내에 존재하므로 메인메모리보다 접근이
빠른 구성을 지닌다. CPU 입장에서의 클럭속도의 차이 때문에 발생하는 병목현상의 부담을 줄인것.
L2캐쉬는 L1보다 크기가 커서 L1캐쉬보다 필요로 하는 데이터를 더 많이 저장할 수 있다.

02 캐쉬와 캐쉬 알고리즘
캐쉬 메모리는 컴퓨터 성능을 향상시키는 데 아주 중요한 역할을 한다.

컴퓨터 프로그램의 일반적인 특성
컴퓨터 프로그램을 관찰해보면 공통적으로 지니는 일반적인 특성이 하나 존재한다.
이러한 공통적인 소프트웨어 특성을 연구하고 분석한 다음, 하드웨어 구조적으로 캐쉬 메모리라는 것을 등장시켜
성능 향상을 가져오게 된 것.

#define ARR_LEN 5
void bubblesort(int srcArr[], int n)
{
	int i, j, temp;
	for(i=0;i<n;i++)
	{
		for(j=1;j<n-1;j++)
		{
			if(srcArr[j-1]>srcArr[j])
			{
				temp = srcArr[j-1];
				srcArr[j-1] = srcArr[j];
				srcArr[j] = temp;
			}
		}
	}
}
int _tmain(int argc, TCHAR** argv)
{
	int arr[ARR_LEN] = { 5,3,7,6,9 };
	bubblesort(arr, ARR_LEN);
	for(int i=0;i<ARR_LEN;i++)
		printf("%d, ", arr[i]);
	return 0;
}
함수 내부적으로 알고리즘 구현에 사용될 세 개의 지역변수 i,j,temp가 선언되어 있다.
대부분의 함수는 특정 연산을 하기 위해 지역변수는 종종 선언된다.
지역변수의 특성은 선언 및 초기화 이후 다양한 값으로 변경도 되고, 값을 얻기 위한 참조도 일어난다.
이러한 특성을 가리켜 Temporal Locality라고 한다.
이런 특성은 프로그램 실행 시 한번 접근이 이뤄진 주소의 메모리 영역은 자주 접근하게 된다는 프로그램
특성을 표현할 때 사용하는 말.
srcArr[j-1]=srcArr[j]에서 j의 값은 0부터 시작해 배열의 크기만큼 증가할 것이다.
여기서도 값을 참조하고 값을 변경하면서 두 번씩 접근하고 있다.
j값이 증가하면, 접근하는 메모리의 주소값은 4바이트씩 증가한다.
이러한 튻성을 Spatial Locality라고 한다.
이 특성은 프로글매 실행 시 접근하는 메모리 영역은 이미 접근이 이루어진 영역의 근처일 확률이 높다는 프로그램
성격을 표현할 때 사용하는 말.
즉 0x12번지 메모리에 접근했다면, 다음 번 메모리 접근은 그 근처일 확률이 높다는 프로그램 성격.
이러한 두 가지 특성 때문에 캐쉬는 성능향상에 많은 도움이 된다.
물론 캐쉬의 도움을 받지 못하도록 프로그램이 구현될 수도 있다.
진정한 개발자는 캐쉬의 도움을 많이 받을 수 있도록 프로그램을 구현해야 한다.
이렇게 구현되는 코드는 Cache Friendly Code 라고 한다.
하지만 굳이 노력하지 않아도 일부로 안사용하게 하지 않는 이상 충분히 캐쉬의 도움을 받게 만들어질 것이다.

캐쉬 알고리즘
자주사용하는 주소의 데이터를 캐쉬에 가져다 놓는것이다.
ALU 연산과정 중에서 필요한 데이터가 있다면 레지스터로 이동시켜야 한다.
필요한 데이터가 0x1000번지에 존재하는 데이터이라 가정.
이 주소에 해당하는 데이터를 레지스터로 가져오기 위해 L1캐쉬에서 데이터가 존재하는 곳을 찾는다.
만약 L1캐쉬에 해당 데이터가 존재할 경우 Cache Hit이 발생했다고 하고 데이터를 레지스터로 이동시킨다.
존재하지 않을 경우 Cache Miss가 발생했다고 하고, L2캐쉬에서 해당 데이터를 가져온다.
L2캐쉬에도 없다면 메인 메모리에서 가져와야 한다.
데이터의 이동은 블록 단위로 진행이 된다.
레지스터로 이동해야 하는 데이터의 주소 번지가 0x1000번지였는데 이 데이터가 L1캐쉬에 존재하지 않고, L2에 
존재한다면 L2캐쉬에서 L1캐쉬로 이동하는 데이터의 단위는 0x1000번지의 데이터를 포함하는 하나의 블록이 된다.
블록 단위로 전송을 해서 Spatial Locality의 특성을 성능 향상에 활용하게 된다.

또한 메모리의 피라미드 구조상 아래로 내려갈수록 블록 크기는 커지게 된다.
이는 아래에 존재하는 메모리일수록 접근 횟수를 줄이는 효과를 가져다 준다.
아래에 존재하는 메모리일수록 속도가 느리기 때문에 접근의 횟수를 줄이는 것이 성능 향상에 많은 도움.

마지막으로 운영체제가 동작을 하고, 프로그램이 실행되는 동안에는 하드 디스크를 제외한 모든 메모리가 항상 채워져 있다.
L2 캐쉬의 경우만 봐도 캐쉬 메모리를 꽉 채워놔야만 L1캐쉬에서 요구하는 데이터를 소유하고 있을 확률이 높아진다.
그러니 캐쉬 메모리를 비워둘 필요가 없는 것.
따라서 L1캐쉬에서 캐쉬미스가 발생해 L2캐쉬로부터 데이터 블록을 읽어들일 때 고민이 발생.
꽉 차 있는 L1 캐쉬에 데이터를 저장하려면 기존에 저장한 데이터를 밀어내야하는데, 잘 정의된 블록 교체
알고리즘에 의해 데이터를 밀어내게 된다.
블록 교체 알고리즘은 Cache's Replacement Policy에 따라서 달라질 수 있는데, 보편적으로 거론되는 것이
LRU(Least-Recently Used)알고리즘이다.
의미대로 가장 오래 전에 참조된 블록을 밀어내는 알고리즘인데, 실제로는 캐쉬 정책에 따라 조금씩 차이가 있다.

Cache Friendly Code 작성기법
int total = 0;
for(int i = 0;i < 10;i++)
{
	for(int j = 0;j < 10;j++)
	{
		total += arr[j][i];
	}
}
배열이 가지고 있는 요소들의 값을 모두 더하고 있습니다.
우선 변수 total의 값이 빈번하게 갱신되고 있다는 측면에서 템퍼럴 로컬리티는 만족이 됩니다.
하지만 스페이셜 로컬리티 특성은 만족하지 않습니다.
즉 새로 접근하는 메모리 영역이 이전에 접근했던 메모리의 주변일 확률이 높다는 특성은 볼 수가 없다.
행단위     열단위
arr[j][i]  arr[i][j]
┌─arr[0][0]─┐
│ arr[0][1]←┘
│ arr[0][2]
└→arr[1][0]
  arr[1][1]
  arr[1][2]
  arr[2][0]
  arr[2][1]
  arr[2][2]
배열의 열 단위 접근방식을 보여주는 데 크기가 작으면 행 단위 접근이든 열 단위 접근이든 스페이셜 로컬리티를 만족.
하지만 배열 크기가 더 커지면 스페이셜 로컬리티를 만족시킬 확률은 낮아진다.
하지만 행 단위 접근방식을 이용하면 배열의 크기에 상관없이 스페이셜 로컬리티를 만족시킨다.

03 가상 메모리(Virtual Memory)
메인 메모리는 512MB인데 어떻게 프로세스에 4GB가 할당되어 프로그램이 실행되는 것일까?

물리 주소(Physical Address)
임베디드 시스템       범용 시스템
ARM9 <--> RAM        CPU <--> RAM <--> 하드디스크
가장 큰 차이점은 하드디스크의 존재 유무이다.
범용 시스템은 하드디스크가 있어 Windows 운영체제에서부터 각종 소프트웨어를 하드디스크에 저장해 놓고,
전원이 인가되면 저장된 소프트웨어를 기반으로 동작하게 된다.
임베디드 시스템은 시스템을 램에 저장할 수 없다.
램에 저장된 데이터는 공급되던 전원이 중단되면 사라지기 때문에 시스템을 동작시키기 위해서는 전원을 인가할 때마다
매번 운영체제를 다시 Loading해야만 한다.
그래서 플래시 메모리로 보완한다.
플래시 메모리에 데이터를 저장해 놓고 전원이 인가되면 램으로 데이터가 이동하도록 디자인.
그런데 이 데이터라는 것은 컴파일이 완료된 운영체제와 그 운영체제를 바탕으로 동작하는 프로그램을 총칭하는 것.

임베디드 시스템의 동작 원리
ARM9 <--> RAM <--> 플래시 메모리 <-- [운영체제+프로그램](컴파일된 바이너리 데이터)
              로딩               로딩
운영체제 코드와 프로그램을 하나의 바이너리 코드로 생성해 시스템에 로딩합니다(전원 인가 시 한번만 로딩).
이것은 운영체제를 먼저 설치해 필요한 프로그램을 설치하는 범용 컴퓨터와 다르다.
만약 램 용량이 16MB라고 하면 ARM9에서 접근가능한 메모리 영역은 0~(16*1024*1024)-1번지 사이가 된다.
이것은 실제 물리적인 메인 메모리의 주소 범위에 해당하며, 이렇게 주소를 할당하는 것을
물리적 주소 지정(Physical Addressing)이라 한다. 
물리적 주소 지정의 특징은 메인 메모리 크기에 따라서 지정 가능한 주소의 범위가 결정되는 것.
물리적 주소 지정을 하게 되면 CPU입장에서는 접근 가능한 주소의 범위가 제한된다.
이것은 프로그래머가 할당할 수 있는 주소 범위가 제한적.
주소의 범위가 제한되면, 프로그래머는 주소 범위를 넘어서지 않도록 주의해야 한다.
주소의 범위에 제한이 생긴다는 것은 엄청난 제약사항.
물리적 주소지정은 메인 메모리 크기 범위 안에서 운영체제와 프로그램을 로딩하고 프로그램 실행과정에서 메모리를
할당해야만 한다.

ARM9은 코어이다. 코어는 CPU의 핵심 모듈로 이 코어를 이용해 다양한 CPU를 디자인하게 된다.

가상 주소(Virtual Address) 시스템 1
32비트 시스템에서 프로세스 생성 시 4GB의 메모리를 할당받을 수 있다.
메인 메모리의 크기는 여기에 비해 부족하고 가상의 주소이다.
가상의 주소를 지정하는 것을 가상 주소 지정(Virtual Addressing)이라 하며, 가상 주소 지정을 통해 할당받게 되는
4GB를 가리켜 가상 메모리 공간(Virtual Address Space)라고 한다.
둘 이상의 프로세스에게도 각각 4GB 메모리 공간 할당이 가능하다.
조금 느린 하드디스크의 여유 공간이 크므로 둘 이상의 프로세스에게 4GB씩 메모리 공간을 할당해 준다고 해서
두가지를 제외하고 문제될 것은 없다.
[첫 번째 문제:선 할당으로 인한 부담]
프로세스를 생성할 때마다 4GB씩 하드디스크 할당해 줄 것인가? 실제 적은 메모리 공간을 프로그램에게 4GB를 미리
할당하는 것은 메모리 낭비도 심하고 시간도 엄청나게 걸리는 작업이다.
[두 번째 문제:느린 속도의 개선 필요성]
하드디스크는 너무 느리다.

가상 메모리 시스템을 이해하면 두 문제를 해결할 수 있다.
가상 메모리 시스템을 구현하는 방법은 표준으로 정해져 있진 않다.
그러나 대부분의 시스템에서 페이징(Paging)이라는 기법을 사용한다.
그리고 페이징 알고리즘의 구현방법도 다양하게 존재한다.
가정1. 16비트 시스템. 0부터 64K-1까지 주소 지정 가능.
가정2. 프로세스별로 64KB 메모리 할당. 가상 메모리 할당.
가정3. 메인 메모리 16KB, 램 용량이 16KB.
이를 32비트 시스템이나 64비트 시스템으로 확장하는 것은 같다.
실제 메모리는 16KB가 전부인데, 프로세스를 생성할 때마다 64KB를 할당해야하고 16K 이상의 메모리는 접근 불가.
프로그램이 실제로 64KB를 전부 사용하지 않을 확률이 꽤 높다는 데에서 힌트를 얻는다.

첫 번째 요청
CPU ------------------> MMU -----> |0 - 4K로 할당| 4KB
1K번지부터 20B할당 요청

두 번째 요청                        |0 - 4K로 할당   | 4KB
CPU ------------------> MMU -----> |32K - 40K로 할당| 4KB
36K번지부터 20B할당 요청            

MMU(Memory Management Unit)는 16KB밖에 존재하지 않는 메모리를 64KB가 존재하는 것처럼 CPU가 느끼도록 컨트롤하는
역할을 한다.
MMU를 CPU와 독립적인 하드웨어로 표시해 뒀지만, 실제로는 CPU와 함께 하나로 패키징되는 장치다.
CPU가 메모리로 직접 접근하지 않고 MMU를 통해 요청을 한다.
"MMU야 1K번지를 시작으로 20바이트를 할당할게."
그러면 MMU는 메인 메모리에서 아직 사용되지 않는 메모리 블록 하나를 골라 할당을 한다.
위에서는 메모리의 할당 단위가 4KB인 셈이다. (정확하게는 범위 끝에 -1해줘야 한다.)
블록의 단위를 16B정도로 최소화하거나, 단위를 아예 없애버리고 필요한 만큼만 할당하면 메모리 사용의 효율성은 증가.
하지만 그만큼 MMU는 복잡한 일을 해야되고 연산이 늘어나고 속도는 감소한다.
12K-13K만 필요한 경우 나머지 13K-16K는 불필요한 할당이 아니고 스페이셜 로컬리티 특성 적용.
블록의 크기 4KB는 시스템에 따라 수백 바이트에서 64KB까지 크기가 다양하다.
이런 블록을 하드웨어 입장에서는 Page Frame이라고하고, 소프트웨어 입장에서는 Page라고 한다.
페이지 프레임은 실제 메인 메모리 블록을 의미하고, 페이지는 가상 메모리 블록을 의미한다.
그리고 페이지 프레임과 페이지의 크기는 일치한다.

가상 메모리                          물리 메모리
60K-64K |4KB| 한개의 Page   ┌------> |0 - 4K로 할당  |
56K-60K |4KB| --------------------->|56 - 60K로 할당|
52K-56K |4KB|       ┌------|------> |8 - 12K로 할당 |
48K-52K |4KB|       |      |     ┌> |16 - 20K로 할당|
        |...|       |      |     |
16K-20K |4KB| -------------|-----┘
12K-16K |4KB|       |      |
08K-12K |4KB| ------┘      |
04K-08K |4KB|              |
00K-04K |4KB| -------------┘
가상 메모리 0K-4K는 물리 메모리의 가장 위쪽에 매핑되어 있다(MMU에 의해 매핑됨).
따라서 CPU가 0K-4K사이에 존재하는 데이터를 요구할 경우 MMU는 매핑된 물리 메모리를 참조해서 데이터를 전송.

CPU에게 서비스를 제공하기 위해서 MMU는 내부적으로 정보를 유지하고 있다.
주목할 부분은 CPU가 요청한 주소의 데이터를 MMU가 해석해 나가는 과정.
페이지 크기를 4KB 기반으로 설명.
	 페이지 테이블                         물리 메모리
	|페이지15|null|              ┌------> |0 - 4K로 할당  |
┌-->|페이지14|    | --------------------->|56 - 60K로 할당|
|	|페이지13|null|       ┌------|------> |8 - 12K로 할당 |
|	|페이지12|null|       |      |     ┌> |16 - 20K로 할당|
|		...              |      |     |
|	|페이지4 |    | -------------|-----┘
|	|페이지3 |null|       |      |
|	|페이지2 |    | ------┘      |
|	|페이지1 |null|              |
|	|페이지0 |    | -------------┘
|
└-------- 1110 00000001
          /         \
페이지프레임 결정  페이지프레임 내의 위치 결정

페이지의 크기를 4KB로 정의하는 경우 64KB의 메모리 공간에서 얻을 수 있는 총 페이지 개수는 16개이다.
페이지0은 0K-4K의 메모리 주소를 나타내며, 페이지1은 4K-8K의 메모리 주소를 나타낸다.
페이지 테이블의 key는 페이지 숫자이다.
그리고 Value은 해당 페이지가 존재하는 페이지 프레임의 시작번지이다.
이러한 테이블 구성을 지닐 경우 페이지 테이블을 참조해서 가상 주소를 실제 할당되어 있는 물리주소로 변환.
예를 들어, CPU는 10진수로 57354번지에 int형 데이터(4바이트) 350을 저장하라는 명령을 MMU에게 내렸다.
그러면 MMU는 일단 가상 주소 57354에 해당하는 물리 주소를 찾아야 한다.
57354는 56K+1번지이기에 14번째 페이지 프레임에 해당한다.
따라서 페이지 테이블을 통해 14번째 프레임의 시작 번지를 확인한다.
57354는 2진수로 111000000001
이 중 상위 4비트 1110만 참조해도 14번째 프레임에 해당한다는 것을 계산할 수 있다.
그러면 이제 14번째 프레임 내에서의 특정 위치를 결정해야 한다.
하위 8비트를 통해 알 수 있다.
마지막 8비트가 00000001이므로 시작 번지 다음이 원하는 위치가 된다.
이 위치를 시작으로 00000100까지 int형 데이터 350이 저장.(4바이트)

페이지 프레임 내에서의 주소 참조
14번째 페이지 프레임
|          | 시작 번지:00000000
  ...
|          | 마지막 번지:11111111

시스템은 기본적으로 4바이트 정렬방식으로 저장이 되므로 1번지에 저장되는 것은 특별한 사례에 해당.

가상 주소(Virtual Address) 시스템 2
메모리 부족문제를 해결해본다.
위와 같은 경우 물리 메모리는 이미 할당이 끝나 더 이상의 메모리 할당이 불가능한 상황.
게다가 둘 이상의 프로세스가 실행된다면 더욱 심각.
부족한 메모리를 해결하는 법은 하드디스크에 있다.
하드디스크도 RAM과 비교해 속도를 제외하면 그 기능에 있어 조금도 부족이 없는 메모리.
하드디스크를 메인 메모리로 확장해서 문제를 해결할 수 있다.
Swap File이라는 개념을 도입해 RAM에 해당하는 메인 메모리를 하드 디스크로까지 확장한 것.

┌─── 메인 메모리 ───┐
|RAM|하드디스크 일부|
       스왑 파일

수십GB 이상의 여유 공간을 메인 메모리로 확장하여 쓸 수 있다는 것은 큰 장점.
메모리 할당 과정에서 부족한 부분은 하드디스크의 여유 공간으로 대체.
큰 용량은 문제를 해결해주지만, 속도 저하라는 또 하나의 문제.
이 속도 저하라는 새로운 문제점은 속도가 빠른 램과 하드디스크의 역할을 완전히 동일시해서 등장한 것.
하드디스크는 스왑 파일을 통해 메인 메모리를 보조해 주는 것이지, 램과 동일한 성격이 아니다.

하드디스크의 역할1
RAM                    스토어    하드 디스크
|0 - 4K로 할당  |       ┌------>|8 - 12K로 할당|
|56 - 60K로 할당|       |
|8 - 12K로 할당 |-------┘
|16 - 20K로 할당|
       ↓상태 변경
|0 - 4K로 할당  |
|56 - 60K로 할당|
|4 - 8K로 할당  |<--- 새로운 할당
|16 - 20K로 할당|
4-8K의 메모리를 할당해야만 한다고 하면 메인 메모리에 여유 공간이 없으니 방법이 없다.
특정 메모리 블록(사용될 확률이 가장 낮은)을 하드디스크에 저장하고 나서 그 영역을 새롭게 할당.
8-12K 영역의 블록을 하드디스크에 저장하고 있다.
이후에 8-12K 영역 접근이 발생하면 다시 꺼내와야 한다.

하드디스크의 역할2
RAM                   Load      하드 디스크
|0 - 4K로 할당  |<--------------|8 - 12K로 할당|
|56 - 60K로 할당| \      
|8 - 12K로 할당 | └------------>|0 - 4K로 할당 |
|16 - 20K로 할당|     Store
	   ↓상태 변경
|8 - 12K로 할당 |
|56 - 60K로 할당|
|4 - 8K로 할당  |
|16 - 20K로 할당|
이전에 저장해 놓은 8-12K 메모리 블록을 다시 램과 가져다 놓기 위해, 0-4K메모리 블록을 하드디스크로 저장.
램과 하드디스크 사이의 데이터 이동 기본 단위는 페이지 프레임 크기와 동일.

이를 바탕으로 둘 이상의 프로세스에게 어떻게 4GB씩 할당이 가능한지 설명.
프로세스 A가 실행을 멈추고 프로세스 B를 실행시키고자 한다고 가정해보자.
현재 메인 메모리 중 RAM에는 프로세스 A를 실행시키기 위한 데이터가 존재할 것.
그런데 이제 프로세스 B를 실행해야만 한다.
그러면 램에 존재하는 프로세스 A의 실행을 위한 데이터 모두를 프로세스 A의 스왑 파일에 저장한다.
그리고 프로세스 B의 실행을 위한 데이터를 프로세스 B의 스왑 파일로부터 램에 가져다 놓는다.
이러한 일련의 과정을 반복하면서 둘 이상의 프로세스가 각각 4GB의 메모리를 할당받아 실행을 이어가는 것.

메인메모리와 MMU
패키징된 CPU
┌──────────────┐
│CPU -----> MMU│
│     요청   | │
└──────────────┘
             |컨트롤
┌────────────────────────────────────┐
│            ↓   Store               │
│RAM <---------------> 하드디스크 일부│
│  Load                              │
└────────────────────────────────────┘
메인 메모리

스왑 파일을 가리켜 Paging 파일이라고도 부른다.

가상 메모리에서 메모리의 할당과 주소 변환에 관련된 내용들만 MMU가 처리한다.
가상 메모리 구성에 필요한 시스템의 기본 기능(메모리 할당 및 변환)은 MMU에 의해 제공되고 이를 바탕으로
Windows 운영체제의 VMM(Virtual Memory Manager)가 사용자 측면 기능을 완성한다.
오늘날의 임베디드 시스템에서도 MMU에 의한 가상 메모리를 지원하는 경우가 많다.

이것만은 알고 갑시다
1. 메모리 계층별 역할과 상호작용 관계를 정리해 보자
컴퓨터 시스템의 메모리는 파라미드 계층 구조를 갖는다. 피라미드의 제일 아래에는 하드디스크가 존재하고, 가장
위에는 레지스터가 존재한다. 피라미드 구조의 아래쪽으로 내려갈수록 CPU와 거리가 멀어서 속도는 느리되, 용량은
엄청난 크기로 증가한다.
2. 캐쉬 메모리가 성능에 도움을 주는 이유
캐쉬 메모리가 성능에 도움을 주는 이유는 프로그램의 일반적인 특성 두 가지 때문. 대부분의 프로그램은 실행과정에서
한번 접근이 이뤄진 주소의 메모리 영역에 자주 접근하는 특성이 있다. 이러한 특성을 가리켜 Temporal Locality.
또한 이미 접근이 이뤄진 영역의 근처에 접근할 확률이 높다는 특성도 지닌다. 이를 Spatial Locality.
이 두 가지 특성 때문에 캐쉬는 성능 향상에 많은 도움이 된다.
3. 가상 메모리
메인 메모리의 부족함을 가상 메모리라는 메커니즘으로 극복하고 있다. 이는 하드디스크를 메인 메모리로 확장하여
완성시킨 개념. 가상 메모리로 인해 Windows상에서 생성되는 모든 프로세스는 메인 메모리의 크기에 상관없이
약속된 메모리 공간을 할당 받을 수 있다. 
4. 피라미드 구조상에서의 캐쉬 관계
전체 메모리의 구조는 피라미드 관계상에서 캐쉬 관계를 형성하고 있다. 캐쉬 메모리가 레지스터와 메인 메모리
사이에서 캐쉬 역할을 하고 있는 것처럼, 메인 메모리도 캐쉬 메모리와 하드디스크 사이에서 캐쉬역할을 하고 있다.
메모리 계층별 관계와 캐쉬 알고리즘의 기본 원리를 기억하자.
5. MMU(Memory Management Unit)
가상 메모리와 실제 물리 메모리 사이에서 주소의 변환을 담당하는 것은 MMU라는 하드웨어 블록이다(물론 소프트웨어로
할 수도 있다). 이는 아주 빈번히 일어나느 연산이기 때문에 하드웨어로 구성해야 성능에 영향을 미치지 않는다.
즉 가상 메모리의 구현은 MMU라는 하드웨어 블록의 도움을 받는다.

*/